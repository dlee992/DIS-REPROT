%!TEX TS-program = XeLaTeX
\documentclass[UTF8]{article}

%--
\usepackage{ctex}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proof}{Proof}[section]

%--
\begin{document}
    
%--
{\flushleft \bf \Large 姓名：} 李达 (程序合成)
{\flushleft \bf \Large 旧学号：} DZ1633007
{\flushleft \bf \Large 新学号：} MG1633116
{\flushleft \bf \Large 日期：} 2020.12.16

%=========================================================================
\section*{论文信息}
\begin{itemize}
    \item Fischer, M. J., Lynch, N. A., and Paterson, M. S. (1985). 
        Impossibility of distributed consensus with one faulty process. \textit{J. ACM, 32(2):374–382.}
    \item Lamport, L. (2001). Paxos made simple. 
    \textit{ACM SIGACT News (Distributed Computing Column) 32, 4 (Whole Number 121, December 2001), pages 51–58.}
    \item Ghemawat, S., Gobioff, H., and Leung, S. T. (2003). The Google file system. 
    \textit{In Proceedings of the nineteenth ACM symposium on Operating systems principles (pp. 29-43).}
    \item Dean, J., and Ghemawat, S. (2004). MapReduce: Simplified data processing on large clusters. \textit{OSDI.}
\end{itemize}
    
%=========================================================================
\section{总序}
前两篇文章是学术性的，从理论层面探讨分布式算法的不可能性边界，以及如何设计一个在特定的分布式场景下达成共识的Paxos算法，这两篇文章都是分布式理论里奠基性的文章。

后两篇文章是工业性的，从2003年的GFS文件系统和MapReduce分布式计算框架开始，工业级生产环境开始实现分布式理论中容错，恢复，高可用的设计思路和具体的策略取舍，
开启了分布式系统工业级实现的先河。

%=========================================================================
\section{Impossibility of Distributed Consensus}
任何理论研究中，想判断一个命题能否被证明，要么正面迎敌，构造性证明它；要么通过反证法来证伪。在分布式理论中，就有一个这样的命题所有人都关心：在分布式的进程之间，
是否存在一个完美的算法，允许进程在通信过程中崩溃，仍能达成共识？比如说，仍能对一个布尔类型的变量达成是true or false的共识？而本文通过精巧的构造性证明和反证法，
完美地证伪：即便只允许一个随机挑选的进程在通信中崩溃，也不存在任何一个算法能够达成对布尔类型值的共识。证明只有短短三四页纸，但这理论成果是卓越的，给未来的分布式共识算法
设计奠定了基调：不存在普世的分布式共识算法，能处理所有的进程崩溃和网络异常，只能在特定的问题场景或者允许的错误场景下，来设计有针对性的共识算法解决某一类特定的问题。

\subsection{建模与证明}
首先假设是完全异步的分布式模型，每个进程拥有基本的send and receive消息的功能，那么整个系统（进程和消息传递）的配置包含每个进程的内部状态和消息缓存里的内容
（正在网络中传输的数据包队列），这里假设没有拜占庭问题。那么按照问题设定，有
\begin{lemma}
    从某个配置$C$开始，两个不相交的调度序列$\sigma _1$，$\sigma_2$分别可以到达两个不同的配置$C_1$，$C_2$，那么两个配置在分别执行调度$\sigma_2$，$\sigma_1$，
    可以到达相同的配置$C_3$，这是根据系统定义自然得出的。
\end{lemma}
引理1是后续定理和引理证明的基础，因此有必要单独表达出来。之后为了证明最终的定理：
\begin{theorem}
    不存在共识协议是完全正确的，即便只有一个进程在通信过程中崩溃（系统启动时还正常，且究竟哪个进程崩溃是不可预测的）。
\end{theorem}
通过反证法证明这个定理，假设存在一个完美的协议$P$能够达成共识（为了一个二值的布尔类型值达成共识），那么：
\begin{lemma}
    协议$P$一定有一个二值的初始配置$C_0$。
\end{lemma}
引理2也是通过反证法来证明。如果不满足引理2，那么$P$一定具有false和true的初始配置。我们定义一种邻接关系：如果只有一个进程的初始值不同，那么两个初始配置就是邻接关系。
那么任何两个初始配置都可以通过一条邻接链连通起来。那么必然存在两个邻接的初始配置，一个是false值，一个是true值。那么假设就是它俩之间唯一不同的进程$p$崩溃了，并且没有发送和接受任何
消息，则最终会达成相同的共识，则其中必有一个是初始配置和最终配置达成共识不同的，这就与我们的假设矛盾，因此得证。

在引理2成立的基础上，可以继续推导：
\begin{lemma}
    假设$C$是$P$的一个二值配置，那么假设事件$e=(p,m)$可以在配置$C$上执行，可以证明，始终能够找到存在一个执行序列，使得执行过事件$e$之后，当前配置仍然是二值的，这就导致
    协议$P$始终无法得出一个共识。
\end{lemma}
引理3的证明过程就是分类讨论，得出最终结论。有了引理3，那么我们可以始终保证协议$P$在特定的执行序列下，始终无法达成共识。则不存在这样的一个完美协议。

%=========================================================================
\section{Paxos Made Simple}
据说Lamport最初写了一篇以“议会”故事描述的分布式共识算法，结果大部分人读后仍然没能明白这算法是如何达成共识，又是如何确保能够达成共识的。
因此，Lamport就重新写了一篇论文，也就是这篇"Paxos Made Simple"，想通过精简短小，更符合分布式理论文章套路的文章来说明它的执行过程和有效性。
但似乎效果依然不好，不然也不会之后诞生了卖点是“Understandbility”的Raft协议了。

我理解这文章之所以不好理解，因为通篇都是抽象描述，没有什么实际例子，而且证明过程大部分自然语言描述，没有严谨的数学证明，让读者难免读完之后依然似懂非懂。
而且，这只是设计分布式协议的基本原则性的介绍，具体如何在实际分布式集群上去实现，具体细节都没有任何介绍，即便是从业者也很难去遵照实现。后面从第四部分的
实际系统设计上也能看出，和Paxos同时代的工业级设计不会考虑那么复杂的分布式场景，一个是程序员开发困难，维护困难，一个是实际场景也没有那么严重的进程或网络崩溃级别。

\subsection{建模与证明}
这里先留白吧，我最后一次报告应该会讨论到Raft协议，到时候再来和Paxos做进一步的对比。

    
%=========================================================================
\section{Google File System and MapReduce}
现在是2020年，读了GFS和MapReduce的文章后，似乎论文本身没有什么新颖之处，但如果放到2003年的历史环境下看，那时候分布式学术理论已经研究了接近20年，但工业级实现一直没有
高标准的范例，而Google正是第一个吃螃蟹的人，发表了一系列的工业级分布式系统设计的文章，而这些系统的复杂程度是以前任何一篇分布式学术论文无法企及的高度，即便没有任何新颖的
技术突破，但能够把性能考量，分区容错，崩溃恢复等内容都很好的实现在一个几千台机器集群上，长期维护，在上层开发各类分布式应用，让人真得相信分布式系统是可靠的，引领了
接下来的分布式系统/框架开发热潮。

\subsection{Google的分布式文件系统}
GFS的产生也是问题驱动的。2003年，Google需要能够存储TB级别的大量数据，而且这些数据的写操作大部分是追加写操作，这就几乎改变了分布式文件系统设计的优化目标。基于机器集群之间
的网络带宽是性能瓶颈和追加写操作占主体的问题驱动，Google的工程师们设计并实现了在当时看来令人异常兴奋的分布式文件系统：GFS。

\subsubsection{GFS文件系统框架}
GFS采用的主从架构，也就是由一个单独的Master节点来控制整个集群的文件系统维护工作，类似于完成单机文件系统的logging，recording metadata这一类核心功能；再将具体的文件存储交个
每个ChunkServer节点，这些节点负责管理好自己要保存那部分文件块，同时定期和Master节点沟通文件是否失效，以及和Client节点沟通是否完成了对应的读写操作。

在这个独立的Master节点上，要维护如下几类信息：维护一个映射关系<filename, array of chunk handles>；维护一个<chunk handle, <list of chunkservers, 
version number, primary chunkserver, lease expiration>>；维护一个logging序列，以及利用checkpoint及时备份metadata信息。
当Client请求读写某个文件时，Master需要根据filename得到chunk handles数组；根据chunk handle得到存储这些chunkserver地址，当前的数据块版本号（以免读到陈旧的数据），
选定一个primary chunkserver作为这多个chunkservers之间的汇总位置，以及当前primary CS的有效期；及时更新Master Replicate的metadata信息，根据后台执行的checkpoint和
log序列，尽快同步。

\subsubsection{Write Operation}
这里我们主要关注GFS如何处理Append Write操作。Client首先向Master发送写请求，Master要为对应的chunk选定一个符合最新备份的primary CS节点，之后增加版本号，
然后告诉对应的primary CS更新版本号，之后让Client直接和chunkserver list进行写操作通信，primary CS会汇总每个CS的写操作结果，最终向Client发送写成功/失败的反馈信息。
在这阶段可能遇到多个Append Write操作并发，GFS选择了一种相对简单的策略来避免发生不一致情况，就是通过Master节点来给写操作排序，每个Write操作都会拿到一段只属于它才可以写入的
文件offset区间，这样即使并发的其他写操作发生了错误，也不会影响到自身写操作的执行。对应的缺点就是文件块中会存在一些padding，最终浪费了一些磁盘空间。
优点就是编码更简单，维护更便捷。


\subsection{MapReduce分布式计算框架}
MR计算框架是搭载在GFS上才能使用的，这就像单机文件系统和基于文件系统功能的用户程序，而MR本身又是一个通用的计算模式的框架，帮助开发上层分布式应用的开发者摆脱自己实现
分布式容错，恢复的泥潭，标准化底层和中间件实现之后，应用开发者只要先判断自己的任务是否合适用MR框架，如果合适，只要专心编写相关的Map和Reduce函数，以及一些额外的配置文件
就能利用上整个分布式集群和MR框架，帮助自己高效完成分布式计算任务。

\subsubsection{MapReduce框架}
可以先把整个基于MR的系统分成三块，底层是GFS分布式文件系统，用于容错，备份，恢复等；中间层是MR框架，用于沟通上层应用和底层文件系统间的数据传输；而上层就是各类应用，
实现具体的Map和Reduce函数。

而应用层的Map函数是实现从文件到<Key,Value>序列生成，而Reduce函数是实现将相同的Key聚集到同一个Reduce计算节点，以便基于同一个Key统计出最终输出结果。
而中间层的基本架构是：一个Master节点用于分派任务，做整体规划；其余多个Map和Reduce节点，用于实际的计算任务执行。
Master节点要负责告知Map节点从哪些服务器节点可以获得对应的输入文件，而Reduce节点应该从哪些Map节点来获取跟自己相关的<Key,Value>数据包，且Reduce节点计算之后，应该把
最终结果继续发送到下一轮Map节点，还是存储到对应的文件块中。

如果某些Map或Reduce节点发生了崩溃，Master节点还要负责重新调度这些没有任务在新的计算节点上重新执行，最终确保任务完成。

而这些崩溃恢复也不可避免的涉及到文件块的冗余备份，因为一个实际的机器节点可能既维护了一些GFS需要的数据块，也在执行一些MR计算任务。

%=========================================================================
\section{总结}
在这些文章之后，接下来会继续读些理论文章，但会更加偏向系统类的文章，了解工业界的实现发展里程。同时，完成一些MIT 6.824分布式系统课程的实现，在模拟的实战中学习。

\end{document}