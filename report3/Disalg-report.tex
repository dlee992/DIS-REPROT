%!TEX TS-program = XeLaTeX
\documentclass[UTF8]{article}

%--
\usepackage{ctex}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proof}{Proof}[section]

%--
\begin{document}
    
%--
{\flushleft \bf \Large 姓名：} 李达 (程序合成)
{\flushleft \bf \Large 旧学号：} DZ1633007
{\flushleft \bf \Large 新学号：} MG1633116
{\flushleft \bf \Large 日期：} 2021.01.10

%=========================================================================
\section*{论文信息}
\begin{itemize}
    \item Ongaro, D., \& Ousterhout, J. (2014). 
    In search of an understandable consensus algorithm. 
    In 2014 {USENIX} Annual Technical Conference ({USENIX}{ATC} 14) (pp. 305-319).
    \item Hunt, P., Konar, M., Junqueira, F. P., \& Reed, B. (2010, June). 
    ZooKeeper: Wait-free Coordination for Internet-scale Systems. 
    In USENIX annual technical conference (Vol. 8, No. 9).
\end{itemize}
    
%=========================================================================
\section{序言}
设计Raft的起因是在它之前，最常用且经典的共识算法是Paxos，虽然相关文献很多，但不管是业内专家还是对分布式感兴趣的普通人，都很难真正理解整个算法的正确性是如何保证的，
关键点在于缺乏直觉的把握和理解，那么实现起来就更加困难了。因此，Raft协议在设计之初，就明确考虑了可理解性，在完成共识算法该做的事情的同时，保证算法是直觉易于接受的，
同时也是实现起来模块化的，不同功能部分更加独立，不会有过多的耦合。

不过，在Raft出现之前，已经有很多工业级别的分布式服务系统出现了，比如ZooKeeper。ZooKeeper系统的设计初衷是提供一种分布式的基础服务，不同的分布式具体应用（也可叫做
Client端）可以按需使用这个基础服务提供的属性，来构建自己的上层分布式功能。ZooKeeper系统由Yahoo!公司研发和开源，已经在公司内部得到广泛的应用，作为搜索引擎的分布式
的基础服务，给多种上层应用作基础技术支撑。

Raft协议作为近十年来最经典的共识算法，ZooKeeper作为较为经典的提供分布式基础服务的工业级系统，都具备典型性，因此本次报告选择这两篇文章来分析和讨论。

%=========================================================================
\section{Raft}
Raft讨论的问题当然比ZooKeeper更加底层，考虑的是如何在多个servers之间达成共识。首先，Raft设计也是基于Replicated State Machine思想，通过记录操作日志，并对日志
进行多机备份，来防止server data diverge，保证所有读写操作之间的线性关系。这是一种很强的一致性要求，这种设计思路更多地考虑是确保Server作为一个整体是始终可用的，
读操作总能读到最近一次的写操作结果，不会读到陈旧值；同时也意味着不需要购买过多的Server来形成这样的整体，因为堆积Server数量，并不能线性地提升系统响应时间。

\subsection{基于Replicated State Machine的基本架构}
复制状态机模型看起来也很简单，构成状态机的每个Server内部有三个核心组成部分：共识模块，操作日志，和状态机本身。当外部Client发来读写请求时，首先经过共识模块的处理，
当大多数Server同意接受当前的Request时，就把该操作添加到日志的尾部，然后再更新状态机（比如一个KV列表）。在基本模型下，要考虑三个最关键的问题：如何进行Leader选举（
确保任何时候都最多只有一个Leader，确保系统启动时或少数Server宕机时，如何重新选举Leader），如何进行日志同步（如何确定对Leader日志加入新的内容，如何确定一个请求
是否被commit，如何同步宕机后重启的Server），如何确保安全（Client之间的写操作顺序得到保证，读操作确保不会读到陈旧的值）。这些都是任何一个共识算法需要应对的挑战。

\subsubsection{Leader Election}
Raft的Leader选举也描述地很简洁，首先给出了Server State的转换图。Server有三种状态：Follower，Candidate，和Leader。首先有个概念要先厘清，共识算法中总会有一个
Version，Term，Index的东西，用来记录当前选举的轮数。一般来讲，都是单调递增的，也就是说，如果一个Server是Leader，但通过通信发现，自身保存的Term比其他某个Server
保存的Term要小，就自动切入到Follower状态。

在系统启动或者Leader宕机的时候，每个Follower都会在停止接受到来自Leader的heartbeat一定时间范围后（在150-300ms之间，具体值随机确定），转入到Candidate状态，
并向其他Server发送投票请求，如果获得多数票，则当选为新一轮的Leader，并更新Term。如果选举时恰好出现平局，则大家都返回到Follower状态，等待超时重新发起选举，因为
Server本身数量不多，且超时时间长度具有随机性，很大概率确保很少出现平局重选的情况。

这就是Leader选举的基本过程。

\subsubsection{Log Replication}
Raft协议在日志同步方面，首先要确保做到不同Server的日志不会出现分叉。当然在特定时间点，允许不同Server的日志里记录未被多数确认的操作记录。而不出现分叉的定义，是以仅
考虑被多数提交的日志内容为前提，对于那么没有被提交却被记录在某个Server日志里的操作，会在后续和Leader同步的过程中被抹除，或者自己当选为Leader，就将这些尚未多数确认的
操作经过沟通变成提交过的日志，并改写与自身不同的Server日志。

结合Raft的Leader选举机制，在日志上还能确保如下属性：
\begin{enumerate}
    \item 如果两个Server各自日志中的两项具有相同的索引（日志计数）和Term（选举轮数），那么它们一定存储着相同的读或写操作内容。
    \item 如果两个Server各自日志中的两项具有相同的索引（日志计数）和Term（选举轮数），那么在这两项之前的所有日志是完全相同的。
\end{enumerate}

这两个性质是很强的一致性要求，当我们进行Leader和Follower之间日志同步时，我们只需要找到最近的一致项即可，之前的项根据属性可以确保相同，对后续项进行同步即可。


\subsubsection{Log Compaction}
日志压缩有一个行业内最简单的处理思路：打快照。问题也很明显，现实系统中，给日志的存储空间不是无限的，而且对于重新加入Server群体的机器来说，从头开始重新执行每个日志，也
不切实际。因此，在合适的时机，合适的机器上对当前的State Machine打快照，将当前数据保存到磁盘中，是一种非常合理的做法。既可以及时清理打快照之前的日志信息，对于新加入的
Server也可以先完整拷贝最新快照，再重新执行日志中的操作。

但打快照显然会遇到一个新问题：在快照的时候，是不是要暂定执行写操作。出于现实的考虑，Raft利用Copy-on-write的操作系统底层机制，当写操作发生时，对局部数据块进行单独拷贝，
这样在保证正确性的前提下，减少对性能的影响。另外一个值得考虑的问题：Leader当然可以执行打快照的操作，那么其他的Followers是否也可以在本地打快照呢？同样出于性能的考虑，
适当减轻Leader的读写负载，Raft也允许Followers打快照，当然前提是针对当前已经确定被commited的日志记录进行快照，对于尾部还未达成共识的日志，不能打到快照中去。


\subsection{耦合程度}
首先，Paxos算法分成四个步骤，几个步骤之间的耦合程度很高，无法脱离其他，单独理解其中一个阶段。但Raft算法耦合程度相对低，每个关键组件可以先单独理解，但同样有些部分不得不
进行耦合，因为单一一个步骤无法保证整个算法的安全性和正确性。这里举出一例：通过在Leader选举的过程中，确保被选举的Leader具有最长的已提交日志，这样省去在日志同步过程中，还要
考虑Follower向Leader输出日志项的情况，这正是一种耦合，不过正是因为这两者的相互作用关系，简化了在日志同步过程中的数据流，只有Leader向Follower输出日志，不会出现
反向输出。

%=========================================================================
\section{ZooKeeper}
ZooKeeper和Raft考虑的问题层面不同，ZooKeeper考虑的是：假定已经有了一种不错的共识算法，那么如何把具体的应用层需要和底层的分布式基础需求隔离开，也就是说，ZooKeeper
作为中间层，提供最基本的分布式原语，而基于原语，可以实现各种各样，功能丰富的分布式需求，比如通过基础原语，可以实现客户端之间的COnfiguration Management、Rendezvous、
Group Membership、Simple Locks、Read/Write Locks等等。在这些拓展的功能基础上，就可以进一步构建企业级的需求，比如Yahoo!的爬虫服务，发布-订阅系统等等。

\subsection{基本架构}
那提供什么样的原语，能够用于构建分布式应用呢？这样的原语能够达到哪种一致性强度？Chubby给出了一种设计思路和实际系统，而ZooKeeper给出了一个类似但不用锁的原语系统。

这套原语系统基于文件系统设计思路，通过目录和文件来管理不同的分布式应用，而客户端可以调用的核心原语API只有7个，主要包含创建、删除文件节点，判断文件是否存在、
读写文件数据、查询目录下的孩子集合、以及sync操作。

显然，不直接提供锁操作，为了实现分布式的锁操作，可以通过这些原语组合来实现。而ZooKeeper强大的一点在于，仅仅通过这7个原语，可以保证两个基本的读写一致性：
\begin{enumerate}
    \item 对于所有的写操作，确保线性，有全序；
    \item 对于单个客户端的读写操作，保证FIFO的顺序，既包含写操作，也包含读操作。
\end{enumerate}

言外之一，就是不保证不同客户端之间读操作的有序性，以及不保证不同客户端之间读操作和写操作的有序性。而之所以产生了这样的读写一致性，恰恰是应用层的需求带来的。
对于搜索引擎和很多分布式应用来说，往往单一时间段的读写请求数量是不均等的，read/write rate可能在10-100之间，那么如果可以放松对read一致性的要求，就有可能
带来服务器数量增长，响应延长接近线性增长的效果。这也是ZooKeeper和Raft在读写一致性上的区别。

\subsection{Wait-free}
ZooKeeper里反复提到了Wait-free的概念，对这个概念的理解我的想法是：仅仅在原语层不提供直接的锁或阻塞机制，但可以通过对原语的组合，来实现锁操作。同时在实际
实现ZooKeeper的系统内部，肯定用到了锁操作，来确保实现刚刚提到的读写一致性，但只是对内使用，对外不暴露这些功能。至于，这样做究竟有趣在哪来，我还需要继续思考。

%=========================================================================
\section{总结}
这段时间，看了更多的分布式应用的文章，看上去这是一个很有趣的领域，有很多奇妙的想法，用来实现各种各样的应用场景需求。今年毕业后，我也可能去工业界从事分布式应用的工作，
希望未来能对分布式系统，原理，架构，以及问题的本源有更多的理解和认识，以及深刻的实践。

\end{document}